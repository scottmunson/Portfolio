{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottmunson/Portfolio/blob/main/privategpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/imartinez/privateGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhkWsZPuTXZz",
        "outputId": "c4c08df8-d96b-4c0b-ec54-4d44669644b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'privateGPT'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 128 (delta 57), reused 47 (delta 43), pack-reused 51\u001b[K\n",
            "Receiving objects: 100% (128/128), 55.94 KiB | 7.99 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# go to the new directory\n",
        "%cd privateGPT/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "198wmutlTouM",
        "outputId": "afb05f3f-dee5-408b-9604-6e9efde6c687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/privateGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7VqEGf-SQiq",
        "outputId": "c93e72d5-0ea6-4bca-a58b-c1e75e079d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.166 (from -r requirements.txt (line 1))\n",
            "  Downloading langchain-0.0.166-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygpt4all==1.1.0 (from -r requirements.txt (line 2))\n",
            "  Downloading pygpt4all-1.1.0.tar.gz (4.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chromadb==0.3.22 (from -r requirements.txt (line 3))\n",
            "  Downloading chromadb-0.3.22-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python==0.1.48 (from -r requirements.txt (line 4))\n",
            "  Downloading llama_cpp_python-0.1.48.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urllib3==1.26.6 (from -r requirements.txt (line 5))\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from -r requirements.txt (line 6))\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (8.2.2)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.166->-r requirements.txt (line 1)) (4.65.0)\n",
            "Collecting pyllamacpp (from pygpt4all==1.1.0->-r requirements.txt (line 2))\n",
            "  Downloading pyllamacpp-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.3/273.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygptj (from pygpt4all==1.1.0->-r requirements.txt (line 2))\n",
            "  Downloading pygptj-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.0/246.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.22->-r requirements.txt (line 3)) (1.5.3)\n",
            "Collecting requests<3,>=2 (from langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hnswlib>=0.7 (from chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading clickhouse_connect-0.5.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.6/922.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=2.2.2 (from chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.22->-r requirements.txt (line 3)) (0.7.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.22->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->-r requirements.txt (line 6)) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->-r requirements.txt (line 6)) (40.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 1)) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.22->-r requirements.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.22->-r requirements.txt (line 3)) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->-r requirements.txt (line 6)) (1.15.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting starlette<0.27.0,>=0.26.1 (from fastapi>=0.85.1->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb==0.3.22->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.3.22->-r requirements.txt (line 3)) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.166->-r requirements.txt (line 1)) (3.4)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (0.15.1+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.166->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.22->-r requirements.txt (line 3)) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->-r requirements.txt (line 6)) (2.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb==0.3.22->-r requirements.txt (line 3)) (3.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (16.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.166->-r requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb==0.3.22->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.22->-r requirements.txt (line 3)) (1.3.0)\n",
            "Building wheels for collected packages: pygpt4all, llama-cpp-python, hnswlib, sentence-transformers\n",
            "  Building wheel for pygpt4all (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygpt4all: filename=pygpt4all-1.1.0-py3-none-any.whl size=5842 sha256=0dbaeca084ffe2b619700beb3b135b1c8bdee10b2b3886016eb90eb5bce7e2e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/e9/c6/7b2548560f0eb92b4c1e4159d8aaeab499240a715d7481e975\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.48-cp310-cp310-linux_x86_64.whl size=184481 sha256=7fce06c2c4894481524b18757bd690772014a6e495ab2fda14bfa5b35851999f\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/06/0e/6ae7b299ed252075128644d31384cac683e9fd768a8538c6be\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119825 sha256=e3b2ce92417a5b21d14d5674ba7effff9cc1946df5a48126f4beb5914ccc78eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=bd01429dd41fe0662c4fe33842ddb769a0ea5d00c52ec4dd1bb20efd302ffbfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built pygpt4all llama-cpp-python hnswlib sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, monotonic, zstandard, websockets, uvloop, urllib3, python-dotenv, pyllamacpp, pygptj, mypy-extensions, multidict, marshmallow, lz4, llama-cpp-python, httptools, hnswlib, h11, frozenlist, backoff, async-timeout, yarl, watchfiles, uvicorn, typing-inspect, starlette, requests, pygpt4all, openapi-schema-pydantic, marshmallow-enum, clickhouse-connect, aiosignal, posthog, pdfminer.six, huggingface-hub, fastapi, dataclasses-json, aiohttp, transformers, langchain, sentence-transformers, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.15\n",
            "    Uninstalling urllib3-1.26.15:\n",
            "      Successfully uninstalled urllib3-1.26.15\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 backoff-2.2.1 chromadb-0.3.22 clickhouse-connect-0.5.24 dataclasses-json-0.5.7 fastapi-0.95.1 frozenlist-1.3.3 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 huggingface-hub-0.14.1 langchain-0.0.166 llama-cpp-python-0.1.48 lz4-4.3.2 marshmallow-3.19.0 marshmallow-enum-1.5.1 monotonic-1.6 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 pdfminer.six-20221105 posthog-3.0.1 pygpt4all-1.1.0 pygptj-2.0.3 pyllamacpp-2.1.3 python-dotenv-1.0.0 requests-2.30.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 starlette-0.26.1 tokenizers-0.13.3 transformers-4.29.1 typing-inspect-0.8.0 urllib3-1.26.6 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 yarl-1.9.2 zstandard-0.21.0\n"
          ]
        }
      ],
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a directory called models\n",
        "!mkdir models"
      ],
      "metadata": {
        "id": "q4Fq1MNBUG_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the models. You should replace the URLs with the ones for the models you're going to use.\n",
        "!wget -P models/ https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin\n",
        "!wget -P models/ https://huggingface.co/Pi3141/alpaca-native-7B-ggml/resolve/397e872bf4c83f4c642317a5bf65ce84a105786e/ggml-model-q4_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zZUaZI8Sr38",
        "outputId": "429e1697-5a41-4d64-db11-2d3e834f3739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-14 17:46:30--  https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin\n",
            "Resolving gpt4all.io (gpt4all.io)... 104.26.1.159, 172.67.71.169, 104.26.0.159, ...\n",
            "Connecting to gpt4all.io (gpt4all.io)|104.26.1.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3785248281 (3.5G)\n",
            "Saving to: ‘models/ggml-gpt4all-j-v1.3-groovy.bin’\n",
            "\n",
            "ggml-gpt4all-j-v1.3 100%[===================>]   3.52G  40.5MB/s    in 97s     \n",
            "\n",
            "2023-05-14 17:48:07 (37.1 MB/s) - ‘models/ggml-gpt4all-j-v1.3-groovy.bin’ saved [3785248281/3785248281]\n",
            "\n",
            "--2023-05-14 17:48:07--  https://huggingface.co/Pi3141/alpaca-native-7B-ggml/resolve/397e872bf4c83f4c642317a5bf65ce84a105786e/ggml-model-q4_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.67.0.34, 18.67.0.55, 18.67.0.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.67.0.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/28/5a/285a8b148ac546626236801f21add8a4ba0da694167d6bb3fb385b2c7cb02f96/9c1bb4808f40aa0059d5343d3aac05fb75d368c240b664878d53d16bf27ade2b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-q4_0.bin%3B+filename%3D%22ggml-model-q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684343638&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzI4LzVhLzI4NWE4YjE0OGFjNTQ2NjI2MjM2ODAxZjIxYWRkOGE0YmEwZGE2OTQxNjdkNmJiM2ZiMzg1YjJjN2NiMDJmOTYvOWMxYmI0ODA4ZjQwYWEwMDU5ZDUzNDNkM2FhYzA1ZmI3NWQzNjhjMjQwYjY2NDg3OGQ1M2QxNmJmMjdhZGUyYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQzNDM2Mzh9fX1dfQ__&Signature=WNeB28p3OHBaanWddvGrWZovKgy0Pe3a4vLsSGHETqkGj3ZQ76bI7HINtYZaQ4a7X-qWoj3fQmX5VIeLFZDCPKCJCdAM-1ue5juJAC6-9LqD%7Evdo%7E7n3zDgIdLII2MBZLquFaxmf4TLgpI72bNpwgo9N-N%7EoFIyxF6544rZOetAL4fYPz7QcOEqGbzUmZsjDu3CZL5ac06qKk38X2b14z0qWrJj7piarUuVafTIUbgdZPdVZnDOXEC-5y5kdWv5NEqjH1OOAba5aYySQ0KlFcY5S4sayTG4sWTzdmRTRzpSj83dn3Nk52uCSm6yq4Tv-YTY7ptoXv0ezfkcYZFR8qg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-14 17:48:07--  https://cdn-lfs.huggingface.co/repos/28/5a/285a8b148ac546626236801f21add8a4ba0da694167d6bb3fb385b2c7cb02f96/9c1bb4808f40aa0059d5343d3aac05fb75d368c240b664878d53d16bf27ade2b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-q4_0.bin%3B+filename%3D%22ggml-model-q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684343638&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzI4LzVhLzI4NWE4YjE0OGFjNTQ2NjI2MjM2ODAxZjIxYWRkOGE0YmEwZGE2OTQxNjdkNmJiM2ZiMzg1YjJjN2NiMDJmOTYvOWMxYmI0ODA4ZjQwYWEwMDU5ZDUzNDNkM2FhYzA1ZmI3NWQzNjhjMjQwYjY2NDg3OGQ1M2QxNmJmMjdhZGUyYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQzNDM2Mzh9fX1dfQ__&Signature=WNeB28p3OHBaanWddvGrWZovKgy0Pe3a4vLsSGHETqkGj3ZQ76bI7HINtYZaQ4a7X-qWoj3fQmX5VIeLFZDCPKCJCdAM-1ue5juJAC6-9LqD%7Evdo%7E7n3zDgIdLII2MBZLquFaxmf4TLgpI72bNpwgo9N-N%7EoFIyxF6544rZOetAL4fYPz7QcOEqGbzUmZsjDu3CZL5ac06qKk38X2b14z0qWrJj7piarUuVafTIUbgdZPdVZnDOXEC-5y5kdWv5NEqjH1OOAba5aYySQ0KlFcY5S4sayTG4sWTzdmRTRzpSj83dn3Nk52uCSm6yq4Tv-YTY7ptoXv0ezfkcYZFR8qg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.64.174.106, 18.64.174.110, 18.64.174.43, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.64.174.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4212727017 (3.9G) [application/octet-stream]\n",
            "Saving to: ‘models/ggml-model-q4_0.bin’\n",
            "\n",
            "ggml-model-q4_0.bin 100%[===================>]   3.92G  94.6MB/s    in 40s     \n",
            "\n",
            "2023-05-14 17:48:48 (100 MB/s) - ‘models/ggml-model-q4_0.bin’ saved [4212727017/4212727017]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rename example.env to .env (if you want to change the settings, you can do so here)\n",
        "!mv example.env .env"
      ],
      "metadata": {
        "id": "ZdKse9spS_aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download any files (.txt, .pdf, or .csv) here that you want to chat with\n",
        "# project comes with US constitution already in the source_documents/ folder\n",
        "# !wget -P source-documents/ {URL OF DOCUMENT}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPSaCmI3VP7h",
        "outputId": "417f53f5-72f6-481b-a8e3-8a8d948c3cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constants.py  LICENSE  privateGPT.py  requirements.txt\n",
            "ingest.py     \u001b[0m\u001b[01;34mmodels\u001b[0m/  README.md      \u001b[01;34msource_documents\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ingest the documents\n",
        "!python ingest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2jOb7tQWjLi",
        "outputId": "7d877629-04e1-4af1-f639-504a7f333267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents from source_documents\n",
            "Loaded 1 documents from source_documents\n",
            "Split into 90 chunks of text (max. 500 tokens each)\n",
            "llama.cpp: loading model from models/ggml-model-q4_0.bin\n",
            "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
            "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 1000\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size = 4113748.20 KB\n",
            "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
            "...................................................................................................\n",
            ".\n",
            "llama_init_from_file: kv self size  = 1000.00 MB\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "Using embedded DuckDB with persistence: data will be stored in: db\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 93177.06 ms /   122 tokens (  763.75 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 93267.25 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 87678.71 ms /   116 tokens (  755.85 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 87759.62 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 70232.13 ms /    96 tokens (  731.58 ms per token)\n",
            "llama_print_timings:        eval time =  1227.81 ms /     1 runs   ( 1227.81 ms per run)\n",
            "llama_print_timings:       total time = 71530.95 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 84546.91 ms /   112 tokens (  754.88 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 84623.31 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 85620.12 ms /   116 tokens (  738.10 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 85696.59 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 86097.65 ms /   114 tokens (  755.24 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 86188.44 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 74171.62 ms /   101 tokens (  734.37 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 74244.07 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 91600.91 ms /   117 tokens (  782.91 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 91679.37 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 52194.15 ms /    71 tokens (  735.13 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 52240.89 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 72686.63 ms /    96 tokens (  757.15 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 72755.42 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 77032.96 ms /   103 tokens (  747.89 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 77106.54 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 87515.78 ms /   119 tokens (  735.43 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 87595.07 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 86388.54 ms /   114 tokens (  757.79 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 86464.31 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 94732.57 ms /   127 tokens (  745.93 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 94816.88 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 95393.12 ms /   127 tokens (  751.13 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 95484.94 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 62926.63 ms /    84 tokens (  749.13 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 62985.75 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 93757.75 ms /   125 tokens (  750.06 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 93848.74 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 77850.29 ms /   104 tokens (  748.56 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 77918.91 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 103428.14 ms /   123 tokens (  840.88 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 103529.21 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 94194.69 ms /   125 tokens (  753.56 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 94281.37 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 81215.02 ms /   109 tokens (  745.09 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 81290.37 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 74170.18 ms /    98 tokens (  756.84 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 74238.55 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 104273.52 ms /   140 tokens (  744.81 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 104377.89 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 70830.69 ms /    95 tokens (  745.59 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 70895.66 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 84829.73 ms /   114 tokens (  744.12 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 84912.33 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 91037.11 ms /   119 tokens (  765.02 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 91121.40 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 83682.96 ms /   112 tokens (  747.17 ms per token)\n",
            "llama_print_timings:        eval time =  1237.62 ms /     1 runs   ( 1237.62 ms per run)\n",
            "llama_print_timings:       total time = 85006.08 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 83240.91 ms /   111 tokens (  749.92 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 83316.03 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 98531.89 ms /   132 tokens (  746.45 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 98626.29 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 105580.93 ms /   134 tokens (  787.92 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 105683.98 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 108695.33 ms /   146 tokens (  744.49 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 108804.10 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 90086.93 ms /   122 tokens (  738.42 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 90172.37 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 98030.14 ms /   130 tokens (  754.08 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 98131.77 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 88746.09 ms /   119 tokens (  745.77 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 88829.57 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 102941.21 ms /   136 tokens (  756.92 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 103032.19 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 100003.36 ms /   133 tokens (  751.90 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 100102.35 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 92375.97 ms /   123 tokens (  751.02 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 92462.44 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 100229.25 ms /   135 tokens (  742.44 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 100318.54 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 87971.32 ms /   116 tokens (  758.37 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 88048.33 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 56644.69 ms /    75 tokens (  755.26 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 56708.32 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 83849.58 ms /   111 tokens (  755.40 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 83926.28 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 88732.58 ms /   120 tokens (  739.44 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 88817.69 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 103715.10 ms /   138 tokens (  751.56 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 103810.76 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 98821.70 ms /   133 tokens (  743.02 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 98911.89 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 88309.90 ms /   117 tokens (  754.79 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 88393.30 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 95379.70 ms /   128 tokens (  745.15 ms per token)\n",
            "llama_print_timings:        eval time =   886.33 ms /     1 runs   (  886.33 ms per run)\n",
            "llama_print_timings:       total time = 96363.96 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 81185.53 ms /   109 tokens (  744.82 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 81267.66 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 94887.65 ms /   127 tokens (  747.15 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 94972.22 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 69623.88 ms /    92 tokens (  756.78 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 69697.63 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 95115.12 ms /   126 tokens (  754.88 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 95210.15 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 98997.76 ms /   132 tokens (  749.98 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 99084.24 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 76346.15 ms /   102 tokens (  748.49 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 76418.22 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 64769.55 ms /    87 tokens (  744.48 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 64831.54 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 85020.06 ms /   115 tokens (  739.30 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 85097.15 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 101920.62 ms /   135 tokens (  754.97 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 102013.26 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 80895.01 ms /   109 tokens (  742.16 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 80975.25 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 94843.68 ms /   127 tokens (  746.80 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 94933.48 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 85782.89 ms /   116 tokens (  739.51 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 85868.64 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 89193.90 ms /   119 tokens (  749.53 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 89273.36 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 95355.51 ms /   128 tokens (  744.96 ms per token)\n",
            "llama_print_timings:        eval time =   885.96 ms /     1 runs   (  885.96 ms per run)\n",
            "llama_print_timings:       total time = 96331.75 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 93205.92 ms /   124 tokens (  751.66 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 93295.21 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 77694.56 ms /   104 tokens (  747.06 ms per token)\n",
            "llama_print_timings:        eval time =   888.11 ms /     1 runs   (  888.11 ms per run)\n",
            "llama_print_timings:       total time = 78666.38 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 71563.52 ms /    96 tokens (  745.45 ms per token)\n",
            "llama_print_timings:        eval time =  1245.39 ms /     1 runs   ( 1245.39 ms per run)\n",
            "llama_print_timings:       total time = 72889.03 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 70585.34 ms /    96 tokens (  735.26 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 70646.63 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 77488.75 ms /   104 tokens (  745.08 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 77561.32 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 75736.15 ms /   101 tokens (  749.86 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 75809.02 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 91473.18 ms /   120 tokens (  762.28 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 91553.39 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 63092.67 ms /    85 tokens (  742.27 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 63156.67 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 36715.59 ms /    50 tokens (  734.31 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 36754.79 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 74234.89 ms /    99 tokens (  749.85 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 74307.45 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 84345.38 ms /   114 tokens (  739.87 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 84422.59 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 89102.74 ms /   118 tokens (  755.11 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 89184.62 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 70385.23 ms /    96 tokens (  733.18 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 70455.14 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 70180.50 ms /    92 tokens (  762.83 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 70245.72 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 76064.79 ms /   103 tokens (  738.49 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 76139.23 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 87004.75 ms /   117 tokens (  743.63 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 87082.48 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 82832.10 ms /   112 tokens (  739.57 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 82910.55 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 65208.59 ms /    87 tokens (  749.52 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 65272.17 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 85663.75 ms /   114 tokens (  751.44 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 85744.20 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 77883.46 ms /   104 tokens (  748.88 ms per token)\n",
            "llama_print_timings:        eval time =   883.96 ms /     1 runs   (  883.96 ms per run)\n",
            "llama_print_timings:       total time = 78840.82 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 91748.26 ms /   122 tokens (  752.03 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 91839.55 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 87908.63 ms /   120 tokens (  732.57 ms per token)\n",
            "llama_print_timings:        eval time =   996.64 ms /     1 runs   (  996.64 ms per run)\n",
            "llama_print_timings:       total time = 88996.38 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 91530.13 ms /   122 tokens (  750.25 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 91610.97 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 100850.35 ms /   134 tokens (  752.61 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 100946.82 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 98305.58 ms /   131 tokens (  750.42 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 98396.26 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 111868.42 ms /   150 tokens (  745.79 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 111970.95 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 79706.58 ms /   107 tokens (  744.92 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 79784.64 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 94534.19 ms /   126 tokens (  750.27 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 94617.14 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 98144.63 ms /   132 tokens (  743.52 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 98234.94 ms\n",
            "\n",
            "llama_print_timings:        load time =  7955.21 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time = 44417.95 ms /    59 tokens (  752.85 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time = 44461.19 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python privateGPT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1nBwia5WyVm",
        "outputId": "a06cf455-f5c1-46c2-c2f5-9c7da5f8e3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama.cpp: loading model from models/ggml-model-q4_0.bin\n",
            "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
            "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 1000\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size = 4113748.20 KB\n",
            "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
            "...................................................................................................\n",
            ".\n",
            "llama_init_from_file: kv self size  = 1000.00 MB\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "Using embedded DuckDB with persistence: data will be stored in: db\n",
            "gptj_model_load: loading model from 'models/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
            "gptj_model_load: n_vocab = 50400\n",
            "gptj_model_load: n_ctx   = 2048\n",
            "gptj_model_load: n_embd  = 4096\n",
            "gptj_model_load: n_head  = 16\n",
            "gptj_model_load: n_layer = 28\n",
            "gptj_model_load: n_rot   = 64\n",
            "gptj_model_load: f16     = 2\n",
            "gptj_model_load: ggml ctx size = 4505.45 MB\n",
            "gptj_model_load: memory_size =   896.00 MB, n_mem = 57344\n",
            "gptj_model_load: ................................... done\n",
            "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
            "\n",
            "Enter a query: how many words in this document?\n",
            "\n",
            "llama_print_timings:        load time =  6182.69 ms\n",
            "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings: prompt eval time =  6180.19 ms /     8 tokens (  772.52 ms per token)\n",
            "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
            "llama_print_timings:       total time =  6187.11 ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/privateGPT/privateGPT.py\", line 57, in <module>\n",
            "    main()\n",
            "  File \"/content/privateGPT/privateGPT.py\", line 42, in main\n",
            "    res = qa(query)    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 140, in __call__\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 134, in __call__\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\", line 119, in _call\n",
            "    docs = self._get_docs(question)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\", line 181, in _get_docs\n",
            "    return self.retriever.get_relevant_documents(question)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/base.py\", line 366, in get_relevant_documents\n",
            "    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\", line 181, in similarity_search\n",
            "    docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\", line 228, in similarity_search_with_score\n",
            "    results = self.__query_collection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/utils.py\", line 50, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\", line 120, in __query_collection\n",
            "    return self._collection.query(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\", line 219, in query\n",
            "    return self._client._query(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/api/local.py\", line 408, in _query\n",
            "    uuids, distances = self._db.get_nearest_neighbors(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/db/clickhouse.py\", line 583, in get_nearest_neighbors\n",
            "    uuids, distances = index.get_nearest_neighbors(embeddings, n_results, ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/db/index/hnswlib.py\", line 230, in get_nearest_neighbors\n",
            "    raise NoIndexException(\n",
            "chromadb.errors.NoIndexException: Index not found, please create an instance before querying\n"
          ]
        }
      ]
    }
  ]
}