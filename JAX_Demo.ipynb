{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottmunson/Portfolio/blob/main/JAX_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX\n",
        "\n",
        "What is Jax? (Taken from their GitHub page)\n",
        "\n",
        "JAX is Autograd and XLA (accelerated linear algebra), brought together for high-performance machine learning research.\n",
        "\n",
        "JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation) via grad as well as forward-mode differentiation, and the two can be composed arbitrarily to any order.\n",
        "\n",
        "JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, jit.\n",
        "\n",
        "GitHub:\n",
        "\n",
        "* https://github.com/google/jax\n",
        "* https://github.com/google/flax"
      ],
      "metadata": {
        "id": "oTd0Vmb-ZO1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "suO4H7VRZ44C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flax jax numpy optax sklearn\n"
      ],
      "metadata": {
        "id": "s7ovZUU4ZWsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basics\n",
        "\n",
        "To illustrate the basic concepts of Jax, one first needs to start with numpy.\n",
        "Numpy runs on the CPU, while JAX runs on the GPU.\n",
        "\n",
        "So for these basics, we are going to do large matrix multiplication."
      ],
      "metadata": {
        "id": "OQwoHfdca5a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Size of the matrices\n",
        "size = 5000\n",
        "\n",
        "# Generating two large matrices\n",
        "matrix1 = np.random.rand(size, size)\n",
        "matrix2 = np.random.rand(size, size)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Performing a matrix multiplication operation\n",
        "result = np.dot(matrix1, matrix2)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f'Time taken for matrix multiplication of size {size}x{size} is {end - start} seconds.')"
      ],
      "metadata": {
        "id": "xSmXLtZka8jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".... and now for JAX:"
      ],
      "metadata": {
        "id": "ehFhNnMscWgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp # This is literally as simple as it gets\n",
        "from jax import random, jit\n",
        "import time\n",
        "\n",
        "# Size of the matrices\n",
        "size = 5000\n",
        "\n",
        "key = random.PRNGKey(0) # Random seed\n",
        "\n",
        "# Generating two large matrices\n",
        "matrix1 = random.normal(key, (size, size)) # Slight differences in how random is used vs np.random\n",
        "matrix2 = random.normal(key, (size, size))\n",
        "\n",
        "# JIT compile the dot operation for efficiency\n",
        "@jit\n",
        "def matmul_jit(A, B):\n",
        "    return jnp.dot(A, B)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Performing a matrix multiplication operation\n",
        "result = matmul_jit(matrix1, matrix2)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f'Time taken for matrix multiplication of size {size}x{size} is {end - start} seconds.')\n"
      ],
      "metadata": {
        "id": "zgzlL-XgcAjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terminology"
      ],
      "metadata": {
        "id": "kHYvdvgOc3tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JAX** = Numerical, GPU-driven, computational library<br>\n",
        "**FLAX** = Machine learning framework made with JAX<br>\n",
        "**JIT** = Just-in-time (JIT) compilation is a concept that comes from the world of computer programming, where it is used to convert bytecode into machine code right before execution. This approach contrasts with ahead-of-time (AOT) compilation, where the bytecode is compiled into machine code before runtime. (*Plagiarised from GPT)"
      ],
      "metadata": {
        "id": "E0Y6a9tNc6l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarities to Numpy"
      ],
      "metadata": {
        "id": "KRIdgDi1d-6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we illustrate the numpy implementation for initialising two matrices and then multiplying them."
      ],
      "metadata": {
        "id": "B7FyT3IMeA0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize two arrays\n",
        "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "y = np.array([[7, 8], [9, 10], [11, 12]])\n",
        "\n",
        "# Perform matrix multiplication\n",
        "z = np.dot(x, y)\n",
        "\n",
        "print(z)\n"
      ],
      "metadata": {
        "id": "sRHJ6cBDeDOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we illustrate the JAX implementation for initialising two matrices and then multiplying them.\n",
        "\n",
        "You will see... they are exactly the same..."
      ],
      "metadata": {
        "id": "jw81ANNceNtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp # The only difference\n",
        "\n",
        "# Initialize two arrays\n",
        "x = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "y = jnp.array([[7, 8], [9, 10], [11, 12]])\n",
        "\n",
        "# Perform matrix multiplication\n",
        "z = jnp.dot(x, y)\n",
        "\n",
        "print(z)\n"
      ],
      "metadata": {
        "id": "vV9wILGXeVrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differences"
      ],
      "metadata": {
        "id": "VNKoDKlcdnjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Device Management\n",
        "\n",
        "Since JAX runs on the GPU, you have to manually move data between the CPU and GPU. This is similar to PyTorch.\n",
        "\n",
        "Take note, if you have a numpy application and want to move to JAX, this is probably the most work you have to do. But the error is simple and recognisable. \n",
        "\n",
        "```sh\n",
        "<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)``\n",
        "```"
      ],
      "metadata": {
        "id": "qdtXn-YEedrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "# Print the list of available devices\n",
        "print(jax.devices())\n",
        "\n",
        "# Create an array on the first available device (which could be a CPU, GPU, or TPU)\n",
        "x = jax.device_put(1.0, device=jax.devices()[0]) # Here we put a scalar value on the GPU\n",
        "\n",
        "# Check which device the array is on\n",
        "print(x.device_buffer.device())"
      ],
      "metadata": {
        "id": "K4WC487ceCxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Immutability\n",
        "\n",
        "The ability to mutate an existing symbol\n",
        "\n",
        "In NumPy, you can modify arrays in-place, for example:"
      ],
      "metadata": {
        "id": "QJ9e7hP6fik6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize an array\n",
        "x = np.array([1, 2, 3])\n",
        "\n",
        "# Modify an element in-place\n",
        "x[0] = 10\n",
        "\n",
        "print(x)  # prints: array([10, 2, 3])\n"
      ],
      "metadata": {
        "id": "mt7exMrmfoN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...however, you can not do this in JAX"
      ],
      "metadata": {
        "id": "iCn98Thefxaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# Initialize an array\n",
        "x = jnp.array([1, 2, 3])\n",
        "\n",
        "# Attempt to modify an element in-place\n",
        "# x[0] = 10  # raises: an error!"
      ],
      "metadata": {
        "id": "3FTnRiE_fz-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...instead, do this:"
      ],
      "metadata": {
        "id": "1ZE5fuidf2gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# Initialize an array\n",
        "x = jnp.array([1, 2, 3])\n",
        "\n",
        "# \"Modify\" the array\n",
        "x.at[0].set(10) # This is the correct way\n",
        "\n",
        "print(x)  # prints: array([10.,  2.,  3.], dtype=float32)"
      ],
      "metadata": {
        "id": "xY56goP4f43r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Features Numpy Doesn't Have\n",
        "\n",
        "*   Automatic Differentiation (Autograd)\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "iGTflJghd5hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Define a simple function\n",
        "def f(x):\n",
        "    return jnp.sin(x)\n",
        "\n",
        "# Compute its gradient\n",
        "df = grad(f) # -> Here we get the gradient. Take note, it is a functor, returning a function to get the gradient, give input\n",
        "\n",
        "# Evaluate the gradient at x = 1.0\n",
        "print(df(1.0))\n"
      ],
      "metadata": {
        "id": "0pDU6mmgcuu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning with Jax"
      ],
      "metadata": {
        "id": "QqBsYewQhjHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dependencies"
      ],
      "metadata": {
        "id": "ZRrXqNKxhpmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import optax # Used for optimisers\n",
        "from jax import numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.training.common_utils import get_metrics, onehot\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "znEVzuLRhfuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load data"
      ],
      "metadata": {
        "id": "0lRrMfMjhts8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X = StandardScaler().fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "num_classes = len(jnp.unique(y_train))"
      ],
      "metadata": {
        "id": "QkTrlaPfhupD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert to JAX arrays"
      ],
      "metadata": {
        "id": "bmcO1ztJh_O1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to JAX arrays\n",
        "X_train = jnp.array(X_train)\n",
        "y_train = onehot(y_train, num_classes)\n",
        "X_test = jnp.array(X_test)\n",
        "y_test = onehot(y_test, num_classes)"
      ],
      "metadata": {
        "id": "FKt8SEyRhvT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "HVdJW2Z8iDc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple Feedforward Neural Network\n",
        "class FFNN(nn.Module):\n",
        "    hidden_units: int\n",
        "    output_units: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.hidden = nn.Dense(self.hidden_units)\n",
        "        self.out = nn.Dense(self.output_units)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = nn.relu(x)\n",
        "        x = self.out(x)\n",
        "        return nn.log_softmax(x)"
      ],
      "metadata": {
        "id": "gK81r_lCiD_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup model, optimiser, loss"
      ],
      "metadata": {
        "id": "_6dD2BUuiRHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network and optimizer\n",
        "learning_rate = 0.01\n",
        "hidden_units = 64\n",
        "output_units = num_classes\n",
        "model = FFNN(hidden_units, output_units)\n",
        "params = model.init(jax.random.PRNGKey(0), X_train[0])\n",
        "optim = optax.adam(learning_rate=learning_rate)\n",
        "tx = train_state.TrainState.create(\n",
        "  apply_fn=model.apply,\n",
        "  params=params,\n",
        "  tx=optim\n",
        ")"
      ],
      "metadata": {
        "id": "Z-KTHoKJiNUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics"
      ],
      "metadata": {
        "id": "4C2rv2P1nbjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(logits, labels):\n",
        "    return -jnp.mean(jnp.sum(labels * logits, axis=-1))"
      ],
      "metadata": {
        "id": "jXv-MWa8nc6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(logits, labels):\n",
        "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == jnp.argmax(labels, -1))\n",
        "    metrics = {'loss': loss, 'accuracy': accuracy}\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "EAsEGdy6nf_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train step"
      ],
      "metadata": {
        "id": "d8r-PY3xlpbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a training step\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(params):\n",
        "        X, y = batch\n",
        "        logits = state.apply_fn(params, X)\n",
        "        loss = -jnp.mean(jnp.sum(logits * y, axis=-1))\n",
        "        return loss, logits\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    metrics = compute_metrics(logits, batch[1])\n",
        "    return state, metrics"
      ],
      "metadata": {
        "id": "TX9Duwk8itFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JIT"
      ],
      "metadata": {
        "id": "VUlo9WcQlt7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_step = jax.jit(train_step)"
      ],
      "metadata": {
        "id": "SsUlvjL3lvoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training loop"
      ],
      "metadata": {
        "id": "SineBl-ilyuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "print(f\"Expected initial loss: {-jnp.log(1/num_classes)}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    permutation = jax.random.permutation(jax.random.PRNGKey(epoch), len(X_train))\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch = X_train[indices], y_train[indices]\n",
        "        tx, metrics = train_step(tx, batch)\n",
        "\n",
        "        print(f\"Epoch {epoch}: accuracy = {metrics['accuracy']*100:.2f}%, loss: {metrics['loss']}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "logits = model.apply(tx.params, X_test)\n",
        "metrics = compute_metrics(logits, y_test)\n",
        "print(f\"Test accuracy: {metrics['accuracy']*100:.2f}%\")"
      ],
      "metadata": {
        "id": "_uXaXBeRlwBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions?"
      ],
      "metadata": {
        "id": "Z-rBGdYjuX8R"
      }
    }
  ]
}